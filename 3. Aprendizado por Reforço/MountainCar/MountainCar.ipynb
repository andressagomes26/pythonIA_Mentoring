{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gymnasium\n",
    "#!pip install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b9b201",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ave_reward_list\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Run Q-learning algorithm\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mQLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Plot Rewards\u001b[39;00m\n\u001b[0;32m     96\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(rewards)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), rewards)\n",
      "Cell \u001b[1;32mIn [5], line 51\u001b[0m, in \u001b[0;36mQLearning\u001b[1;34m(env, learning, discount, epsilon, min_eps, episodes)\u001b[0m\n\u001b[0;32m     48\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Get next state and reward\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m state2, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Discretize state2\u001b[39;00m\n\u001b[0;32m     54\u001b[0m state2_adj \u001b[38;5;241m=\u001b[39m (state2 \u001b[38;5;241m-\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mlow)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.savefig('rewards.jpg')     \n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tqdm\n",
    "\n",
    "# ambiente \n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "# seed\n",
    "seed= 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "esp=np.finfo(np.float32).eps.item()\n",
    "print('Espaço de ação: ', env.observation_space.low) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_hidden_units=128\n",
    "\n",
    "model=AtorCritico(num_actions,num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "num_states\n",
    "\n",
    "Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "ave_reward_list = []\n",
    "epsilon = 0.8\n",
    "min_eps = 0\n",
    "episodes = 1000\n",
    "# Calculate episodic reduction in epsilon\n",
    "reduction = (epsilon - min_eps)/episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Q learning algorithm\n",
    "for i in range(episodes):\n",
    "    # Initialize parameters\n",
    "    done = False\n",
    "    tot_reward, reward = 0,0\n",
    "    state = env.reset()\n",
    "    \n",
    "    state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a73b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a42592",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Discretize state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8251f5",
   "metadata": {},
   "source": [
    "### novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d50a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtorCritico(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_actions:int,\n",
    "        num_hidden_units:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.common = layers.Dense(num_hidden_units, activation='relu')\n",
    "        self.ator = layers.Dense(num_actions)\n",
    "        self.critico =layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs:tf.Tensor) -> tuple[tf.Tensor,tf.Tensor]:\n",
    "        x= self.common(inputs)\n",
    "        return self.ator(x), self.critico(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add66d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space() #.n teste\n",
    "num_hidden_units=128\n",
    "\n",
    "model=AtorCritico(num_actions,num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_steps(action: np.ndarray)-> tuple[np.ndarray,np.ndarray,np.ndarray]:\n",
    "    estado, recompenca, final, truncado, info=env.step(action)\n",
    "    return (estado.astype(np.float32), np.array(recompenca, np.int32), np.array(final, np.int32))\n",
    "\n",
    "def tf_env_steps(action: tf.Tensor)-> list[tf.Tensor]:\n",
    "    return tf.numpy_function(env_steps, [action],[tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rodar_ep(initial_state:tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    max_steps:int)-> tuple[tf.Tensor,tf.Tensor,tf.Tensor ]:\n",
    "\n",
    "    acao_probs= tf.TensorArray(dtype=tf.float32,size=0, dynamic_size=True)\n",
    "    valores= tf.TensorArray(dtype=tf.float32,size=0, dynamic_size=True)\n",
    "    recompencas= tf.TensorArray(dtype=tf.int32,size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape=initial_state.shape\n",
    "    estado=initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        estado=tf.expand_dims(estado, 0)\n",
    "\n",
    "        action_logists_t, value =model(estado)\n",
    "\n",
    "        acao= tf.random.categorical(action_logists_t, 1 )[0,0]\n",
    "        acao_probs_t= tf.nn.softmax(action_logists_t)\n",
    "\n",
    "        valores =valores.write(t, tf.squeeze(value))\n",
    "\n",
    "        acao_probs= acao_probs.write(t, acao_probs_t[0, acao])\n",
    "\n",
    "        estado, recompenca, final =tf_env_steps(acao)\n",
    "        estado.set_shape(initial_state_shape)\n",
    "\n",
    "        recompencas=recompencas.write(t,recompenca)\n",
    "\n",
    "        if tf.cast(final, tf.bool):\n",
    "            break\n",
    "    acoes_prob=acao_probs.stack()\n",
    "    valores= valores.stack()\n",
    "    recompencas= recompencas.stack()\n",
    "\n",
    "    return acoes_prob,valores,recompencas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507794c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receber_valor_esperado(\n",
    "    recompencas: tf.Tensor,\n",
    "    gamma: float,\n",
    "    standardize: bool=True\n",
    ")-> tf.Tensor:\n",
    "    n=tf.shape(recompencas)[0]\n",
    "    returns= tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    recompencas= tf.cast(recompencas[::-1], dtype=tf.float32)\n",
    "    soma_descontada= tf.constant(0.0)\n",
    "    soma_descontada_shape=soma_descontada.shape\n",
    "    for i in tf.range(n):\n",
    "        recompenca=recompencas[i]\n",
    "        soma_descontada= recompenca+gamma*soma_descontada\n",
    "        soma_descontada.set_shape(soma_descontada_shape)\n",
    "        returns= returns.write(i, soma_descontada)\n",
    "    returns= returns.stack()[::-1]\n",
    "    if standardize:\n",
    "        returns= ((returns-tf.math.reduce_mean(returns))/\n",
    "        (tf.math.reduce_std(returns)+esp))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa55bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss= tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def computar_perdas(\n",
    "    action_prob:tf.Tensor,\n",
    "    valores: tf.Tensor,\n",
    "    returns: tf.Tensor\n",
    ")-> tf.Tensor:\n",
    "    vantagem= returns-valores\n",
    "\n",
    "    action_log_prob=tf.math.log(action_prob)\n",
    "    ator_loss= -tf.math.reduce_sum(action_log_prob*vantagem)\n",
    "\n",
    "    critico_loss= huber_loss(valores,returns)\n",
    "\n",
    "    return ator_loss+critico_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "otimizador= tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    otimizador: tf.keras.optimizers.Optimizer,\n",
    "    gamma:float,\n",
    "    num_max_steps_por_ep:int)-> tf.Tensor:\n",
    "    with tf.GradientTape() as tape:\n",
    "        acoes_prob, valores,recompencas= rodar_ep(initial_state, model,num_max_steps_por_ep)\n",
    "\n",
    "        retornos= receber_valor_esperado(recompencas, gamma)\n",
    "\n",
    "        acoes_prob, valores, retornos=[tf.expand_dims(x,1) for x in [acoes_prob, valores, retornos]]\n",
    "\n",
    "        perda= computar_perdas(acoes_prob, valores, retornos)\n",
    "    grads= tape.gradient(perda, model.trainable_variables)\n",
    "\n",
    "    otimizador.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    recompenca_do_ep=tf.math.reduce_sum(recompencas)\n",
    "\n",
    "    return recompenca_do_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action: int):\n",
    "        assert self.action_space.contains(\n",
    "            action\n",
    "        ), f\"{action!r} ({type(action)}) invalid\"\n",
    "\n",
    "        position, velocity = self.state\n",
    "        velocity += (action - 1) * self.force + math.cos(3 * position) * (-self.gravity)\n",
    "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
    "        position += velocity\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        if position == self.min_position and velocity < 0:\n",
    "            velocity = 0\n",
    "\n",
    "        terminated = bool(\n",
    "            position >= self.goal_position and velocity >= self.goal_velocity\n",
    "        )\n",
    "        reward = -1.0\n",
    "\n",
    "        self.state = (position, velocity)\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "min_ep=100\n",
    "max_ep= 10000\n",
    "max_steps_per_ep=500\n",
    "\n",
    "limiar_de_recompenca=-100#475 - teste\n",
    "recompenca_rodar=0\n",
    "\n",
    "gamma=.99\n",
    "\n",
    "recompancas_do_ep: collections.deque=collections.deque(maxlen=min_ep)\n",
    "\n",
    "t= tqdm.trange(max_ep)\n",
    "for i in t:\n",
    "    estado_inicial, info = env.reset()\n",
    "    estado_inicial= tf.constant(estado_inicial, dtype=tf.float32)\n",
    "    recompanca_do_ep= int(train_step(\n",
    "        estado_inicial,model, otimizador, gamma, max_steps_per_ep)\n",
    "    )\n",
    "\n",
    "    recompancas_do_ep.append(recompanca_do_ep)\n",
    "    recompenca_rodar= statistics.mean(recompancas_do_ep)\n",
    "\n",
    "    t.set_postfix(recompanca_do_ep=recompanca_do_ep, recompenca_rodar=recompenca_rodar)\n",
    "    if recompenca_rodar > limiar_de_recompenca and i > min_ep:\n",
    "        break\n",
    "print(f'\\n EP:{i} \\n recompanca media {recompenca_rodar:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(1000): \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recompancas_do_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab625cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import  display as ipythondisplay\n",
    "from PIL import Image\n",
    "\n",
    "render_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "def render_ep(env: gym.Env, model: tf.keras.Model, max_steps:int):\n",
    "    state, info= render_env.reset()\n",
    "    state - tf.constant(state, dtype= tf.float32)\n",
    "    screen= render_env.render()\n",
    "    images= [Image.fromarray(screen)]\n",
    "\n",
    "    for i in range(1, max_steps+1):\n",
    "        state= tf.expand_dims(state, 0)\n",
    "        action_probs, _ = model(state)\n",
    "        action= np.argmax(np.squeeze( action_probs))\n",
    "\n",
    "        state, recompanca, final, truncado, info = render_env.step(action)\n",
    "        state= tf.constant(state,dtype=tf.float32)\n",
    "\n",
    "        if i%10==0:\n",
    "            screen= render_env.render()\n",
    "            images.append(Image.fromarray(screen))\n",
    "\n",
    "        if final:\n",
    "            break\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96727702",
   "metadata": {},
   "outputs": [],
   "source": [
    "images= render_ep(env, model, max_ep) \n",
    "images[0].save('MountainCar.gif',save_all=True, append_images=images[1:],loop=0, duration=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
